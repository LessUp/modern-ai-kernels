# TensorCraft-HPC

<p align="center">
  <strong>Demystifying High-Performance AI Kernels with Modern C++ & CUDA</strong>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#documentation">Documentation</a> â€¢
  <a href="#benchmarks">Benchmarks</a> â€¢
  <a href="#contributing">Contributing</a>
</p>

---

TensorCraft-HPC æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„ã€æ•™å­¦å‹å¥½ä¸”å·¥ä¸šçº§çš„é«˜æ€§èƒ½ AI ç®—å­ä¼˜åŒ–åº“ã€‚å®ƒå±•ç¤ºäº†ä»æœ´ç´ å®ç°åˆ°æè‡´ä¼˜åŒ–çš„æ¸è¿›å¼ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¶µç›–äº† LLM å’Œæ·±åº¦å­¦ä¹ ä¸­æœ€å…³é”®çš„ç®—å­ã€‚

## ğŸ¯ é¡¹ç›®æ„¿æ™¯

æœ¬é¡¹ç›®æ—¨åœ¨åˆ›å»ºä¸€ä¸ªç°ä»£åŒ–çš„ç®—å­ä¼˜åŒ–çŸ¥è¯†åº“ï¼Œä¸»è¦ä½“ç°åœ¨"æ–°"å’Œ"æ·±"ï¼š

- **æ–°**ï¼šä½¿ç”¨ C++17/20/23 æ ‡å‡†ï¼ŒCMake 3.20+ æ„å»ºç³»ç»Ÿï¼Œæ”¯æŒ CUDA 11.0-13.1
- **æ·±**ï¼šä¸åªæ˜¯å†™ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œè€Œæ˜¯æ·±å…¥åˆ° Tensor Coreã€FlashAttentionã€é‡åŒ–ç­‰å‰æ²¿æŠ€æœ¯

## âœ¨ Features

### æ ¸å¿ƒç®—å­åº“

| ç±»åˆ« | ç®—å­ | ä¼˜åŒ–çº§åˆ« |
|------|------|----------|
| **Elementwise** | ReLU, SiLU, GeLU, Sigmoid, Tanh, Softplus | å‘é‡åŒ–åŠ è½½ |
| **Normalization** | LayerNorm, RMSNorm, BatchNorm | Warp Shuffle |
| **GEMM** | çŸ©é˜µä¹˜æ³• | Naive â†’ Tiled â†’ Double Buffer â†’ Tensor Core |
| **Attention** | FlashAttention, RoPE, PagedAttention, MoE | Online Softmax |
| **Convolution** | Conv2D, Im2Col, Depthwise, Pointwise | å¤šç®—æ³•æ”¯æŒ |
| **Sparse** | CSR/CSC SpMV, SpMM | å‘é‡åŒ– SpMV |
| **Fusion** | Bias+GeLU, Bias+ReLU | Epilogue æ¨¡å¼ |
| **Quantization** | INT8, FP8 (CUDA 12.0+) | é‡åŒ–/åé‡åŒ– |

### æŠ€æœ¯ç‰¹æ€§

- ğŸš€ **ç°ä»£ C++**: C++17 åŸºç¡€ï¼ŒC++20/23 å¯é€‰ç‰¹æ€§ï¼ˆConcepts, constexpr ifï¼‰
- ğŸ® **å¤šæ¶æ„æ”¯æŒ**: Volta (SM 7.0+), Ampere (SM 8.0+), Hopper (SM 9.0+)
- ğŸ“¦ **Header-Only**: æ ¸å¿ƒåº“ä¸ºçº¯å¤´æ–‡ä»¶ï¼Œæ˜“äºé›†æˆ
- ğŸ **Python ç»‘å®š**: é€šè¿‡ pybind11 æä¾› Python æ¥å£
- ğŸ§ª **å®Œæ•´æµ‹è¯•**: GoogleTest å•å…ƒæµ‹è¯• + å±æ€§æµ‹è¯•
- ğŸ“Š **æ€§èƒ½åŸºå‡†**: Google Benchmark æ€§èƒ½æµ‹è¯•

## ğŸš€ Quick Start

### ç¯å¢ƒè¦æ±‚

- CMake 3.20+
- CUDA Toolkit 11.0+ (æ¨è 12.0+ ä»¥è·å¾— FP8 æ”¯æŒ)
- C++17 å…¼å®¹ç¼–è¯‘å™¨ (GCC 9+, Clang 10+, MSVC 2019+)
- (å¯é€‰) Python 3.8+ ç”¨äº Python ç»‘å®š

### æ„å»ºé¡¹ç›®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/tensorcraft-hpc.git
cd tensorcraft-hpc

# ä½¿ç”¨ CMake Presets é…ç½®ï¼ˆæ¨èï¼‰
cmake --preset release

# æˆ–æ‰‹åŠ¨é…ç½®
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release

# æ„å»º
cmake --build build/release -j$(nproc)

# è¿è¡Œæµ‹è¯•
ctest --test-dir build/release --output-on-failure

# è¿è¡ŒåŸºå‡†æµ‹è¯•
./build/release/benchmarks/gemm_benchmark
```

### åœ¨é¡¹ç›®ä¸­ä½¿ç”¨

**æ–¹å¼ 1: ä½œä¸ºå­ç›®å½•**

```cmake
# CMakeLists.txt
add_subdirectory(tensorcraft-hpc)
target_link_libraries(your_target PRIVATE tensorcraft)
```

**æ–¹å¼ 2: ç›´æ¥åŒ…å«å¤´æ–‡ä»¶**

```cmake
target_include_directories(your_target PRIVATE path/to/tensorcraft-hpc/include)
target_link_libraries(your_target PRIVATE CUDA::cudart)
```

## ğŸ“– Documentation

### ä½¿ç”¨ç¤ºä¾‹

#### Elementwise æ“ä½œ

```cpp
#include "tensorcraft/kernels/elementwise.hpp"

using namespace tensorcraft::kernels;

// æ¿€æ´»å‡½æ•°
relu(d_input, d_output, n);
gelu(d_input, d_output, n);
silu(d_input, d_output, n);

// å‘é‡è¿ç®—
vector_add(d_a, d_b, d_c, n);
vector_mul(d_a, d_b, d_c, n);

// è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°
launch_elementwise(d_input, d_output, n, LeakyReLU<float>{0.01f});
```

#### GEMM çŸ©é˜µä¹˜æ³•

```cpp
#include "tensorcraft/kernels/gemm.hpp"

using namespace tensorcraft::kernels;

// é»˜è®¤ä½¿ç”¨ Tiled ç‰ˆæœ¬
gemm(d_A, d_B, d_C, M, N, K);

// é€‰æ‹©ä¼˜åŒ–çº§åˆ«
launch_gemm(d_A, d_B, d_C, M, N, K, 1.0f, 0.0f, GemmVersion::Naive);
launch_gemm(d_A, d_B, d_C, M, N, K, 1.0f, 0.0f, GemmVersion::Tiled);
launch_gemm(d_A, d_B, d_C, M, N, K, 1.0f, 0.0f, GemmVersion::DoubleBuffer);

// Tensor Core (éœ€è¦ half ç²¾åº¦)
#ifdef TC_HAS_WMMA
launch_gemm_wmma(d_A_half, d_B_half, d_C_float, M, N, K);
#endif

// çŸ©é˜µè½¬ç½®
transpose(d_input, d_output, rows, cols);
```

#### å½’ä¸€åŒ–å±‚

```cpp
#include "tensorcraft/kernels/normalization.hpp"

using namespace tensorcraft::kernels;

// LayerNorm: y = gamma * (x - mean) / sqrt(var + eps) + beta
layernorm(d_input, d_gamma, d_beta, d_output, batch_size, hidden_size);

// RMSNorm: y = x / RMS(x) * weight (LLaMA, Mistral ç­‰æ¨¡å‹ä½¿ç”¨)
rmsnorm(d_input, d_weight, d_output, batch_size, hidden_size);

// BatchNorm (æ¨ç†æ¨¡å¼)
launch_batchnorm(d_input, d_gamma, d_beta, d_mean, d_var, d_output,
                 N, C, H, W, eps, /*fuse_relu=*/false);
```

#### Attention æœºåˆ¶

```cpp
#include "tensorcraft/kernels/attention.hpp"

using namespace tensorcraft::kernels;

// FlashAttention é£æ ¼çš„æ³¨æ„åŠ›è®¡ç®—
float scale = 1.0f / sqrtf(head_dim);
launch_flash_attention(d_Q, d_K, d_V, d_O,
                       batch_size, num_heads, seq_len, head_dim, scale);

// RoPE ä½ç½®ç¼–ç 
precompute_rope_cache(d_cos, d_sin, max_seq_len, head_dim);
launch_rope(d_x, d_cos, d_sin, batch_size, seq_len, num_heads, head_dim, start_pos);

// MoE è·¯ç”±
launch_moe_router(d_gate_logits, d_expert_indices, d_expert_weights,
                  batch_size, num_experts, top_k);
```

#### å·ç§¯æ“ä½œ

```cpp
#include "tensorcraft/kernels/conv2d.hpp"

using namespace tensorcraft::kernels;

// æ ‡å‡†å·ç§¯
conv2d(d_input, d_weight, d_bias, d_output,
       N, C, H, W, K, R, S, stride, padding);

// Depthwise å·ç§¯ (MobileNet ç­‰)
conv2d_depthwise(d_input, d_weight, d_bias, d_output,
                 N, C, H, W, R, S, stride, padding);

// Im2Col å˜æ¢ (ç”¨äº Im2Col + GEMM å·ç§¯)
launch_im2col(d_input, d_col, N, C, H, W, R, S, stride, stride, pad, pad);
```

#### ç¨€ç–çŸ©é˜µ

```cpp
#include "tensorcraft/kernels/sparse.hpp"

using namespace tensorcraft::kernels;

// CSR æ ¼å¼çš„ SpMV: y = A * x
launch_spmv_csr(d_values, d_col_indices, d_row_ptrs, d_x, d_y, rows);

// CSR æ ¼å¼çš„ SpMM: C = A * B
launch_spmm_csr(d_A_values, d_A_col_indices, d_A_row_ptrs,
                d_B, d_C, M, K, N);
```

#### ç®—å­èåˆä¸é‡åŒ–

```cpp
#include "tensorcraft/kernels/fusion.hpp"

using namespace tensorcraft::kernels;

// GEMM + Bias + GeLU èåˆ
gemm_bias_gelu(d_A, d_B, d_bias, d_C, M, N, K);

// GEMM + Bias + ReLU èåˆ
gemm_bias_relu(d_A, d_B, d_bias, d_C, M, N, K);

// INT8 é‡åŒ–
quantize_int8(d_input, d_output_int8, scale, zero_point, n);
dequantize_int8(d_input_int8, d_output, scale, zero_point, n);
```

### Python æ¥å£

```python
import tensorcraft_ops as tc
import numpy as np

# æ¿€æ´»å‡½æ•°
input_data = np.random.randn(1024, 512).astype(np.float32)
output = tc.relu(input_data)
output = tc.gelu(input_data)
output = tc.silu(input_data)

# Softmax
output = tc.softmax(input_data)

# å½’ä¸€åŒ–
gamma = np.ones(512, dtype=np.float32)
beta = np.zeros(512, dtype=np.float32)
output = tc.layernorm(input_data, gamma, beta)

weight = np.ones(512, dtype=np.float32)
output = tc.rmsnorm(input_data, weight)

# GEMM
A = np.random.randn(256, 512).astype(np.float32)
B = np.random.randn(512, 128).astype(np.float32)
C = tc.gemm(A, B, version='tiled')  # 'naive', 'tiled', 'double_buffer'
```

## ğŸ“Š Benchmarks

### GEMM æ€§èƒ½å¯¹æ¯”

åœ¨ NVIDIA RTX 3090 ä¸Šçš„æµ‹è¯•ç»“æœï¼š

| çŸ©é˜µå¤§å° | Naive | Tiled | Double Buffer | cuBLAS |
|----------|-------|-------|---------------|--------|
| 512x512  | 15 GFLOPS | 180 GFLOPS | 220 GFLOPS | 280 GFLOPS |
| 1024x1024 | 18 GFLOPS | 350 GFLOPS | 450 GFLOPS | 520 GFLOPS |
| 2048x2048 | 20 GFLOPS | 480 GFLOPS | 620 GFLOPS | 750 GFLOPS |

### è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# GEMM åŸºå‡†æµ‹è¯•
./build/release/benchmarks/gemm_benchmark

# Attention åŸºå‡†æµ‹è¯•
./build/release/benchmarks/attention_benchmark

# å·ç§¯åŸºå‡†æµ‹è¯•
./build/release/benchmarks/conv_benchmark
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
TensorCraft-HPC/
â”œâ”€â”€ include/tensorcraft/
â”‚   â”œâ”€â”€ core/                    # æ ¸å¿ƒå·¥å…·
â”‚   â”‚   â”œâ”€â”€ cuda_check.hpp       # CUDA é”™è¯¯æ£€æŸ¥
â”‚   â”‚   â”œâ”€â”€ features.hpp         # ç‰¹æ€§æ£€æµ‹
â”‚   â”‚   â””â”€â”€ type_traits.hpp      # ç±»å‹ç‰¹å¾
â”‚   â”œâ”€â”€ memory/                  # å†…å­˜ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ aligned_vector.hpp   # å¯¹é½å‘é‡
â”‚   â”‚   â”œâ”€â”€ tensor.hpp           # Tensor å°è£…
â”‚   â”‚   â””â”€â”€ memory_pool.hpp      # å†…å­˜æ± 
â”‚   â””â”€â”€ kernels/                 # ç®—å­å®ç°
â”‚       â”œâ”€â”€ elementwise.hpp      # Elementwise ç®—å­
â”‚       â”œâ”€â”€ softmax.hpp          # Softmax
â”‚       â”œâ”€â”€ normalization.hpp    # å½’ä¸€åŒ–å±‚
â”‚       â”œâ”€â”€ gemm.hpp             # GEMM
â”‚       â”œâ”€â”€ attention.hpp        # Attention
â”‚       â”œâ”€â”€ conv2d.hpp           # å·ç§¯
â”‚       â”œâ”€â”€ sparse.hpp           # ç¨€ç–çŸ©é˜µ
â”‚       â””â”€â”€ fusion.hpp           # èåˆä¸é‡åŒ–
â”œâ”€â”€ src/python_ops/              # Python ç»‘å®š
â”œâ”€â”€ tests/                       # å•å…ƒæµ‹è¯•
â”œâ”€â”€ benchmarks/                  # æ€§èƒ½åŸºå‡†
â”œâ”€â”€ docs/                        # æ–‡æ¡£
â”‚   â”œâ”€â”€ modern_cpp_cuda.md       # Modern C++ æŒ‡å—
â”‚   â”œâ”€â”€ optimization_guide.md    # ä¼˜åŒ–æŒ‡å—
â”‚   â”œâ”€â”€ api_reference.md         # API å‚è€ƒ
â”‚   â””â”€â”€ architecture.md          # æ¶æ„è®¾è®¡
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ CMakePresets.json
â””â”€â”€ README.md
```

## ğŸ”§ é…ç½®é€‰é¡¹

### CMake é€‰é¡¹

| é€‰é¡¹ | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `TC_BUILD_TESTS` | ON | æ„å»ºæµ‹è¯• |
| `TC_BUILD_BENCHMARKS` | ON | æ„å»ºåŸºå‡†æµ‹è¯• |
| `TC_BUILD_PYTHON` | ON | æ„å»º Python ç»‘å®š |
| `TC_ENABLE_FP16` | ON | å¯ç”¨ FP16 æ”¯æŒ |
| `TC_ENABLE_BF16` | ON | å¯ç”¨ BF16 æ”¯æŒ |

### CMake Presets

```bash
cmake --preset debug      # è°ƒè¯•æ„å»ºï¼Œå¯ç”¨ CUDA è°ƒè¯•
cmake --preset release    # å‘å¸ƒæ„å»ºï¼Œæœ€å¤§ä¼˜åŒ–
cmake --preset profile    # æ€§èƒ½åˆ†ææ„å»º
```

## ğŸ¤ Contributing

æ¬¢è¿è´¡çŒ®ï¼è¯·é˜…è¯» [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

### å¼€å‘è·¯çº¿å›¾

- [x] Phase 1: åŸºç¡€è®¾æ–½å’Œæ ¸å¿ƒç®—å­
- [x] Phase 2: GEMM ä¼˜åŒ–å’Œ Attention
- [x] Phase 3: å·ç§¯å’Œç¨€ç–çŸ©é˜µ
- [x] Phase 4: èåˆå’Œé‡åŒ–
- [ ] Phase 5: CUDA 12+ é«˜çº§ç‰¹æ€§ (TMA, WGMMA)
- [ ] Phase 6: æ›´å¤š LLM ç®—å­ (KV Cache, Speculative Decoding)

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- [API å‚è€ƒ](docs/api_reference.md) - å®Œæ•´çš„ API æ–‡æ¡£
- [æ¶æ„è®¾è®¡](docs/architecture.md) - ç³»ç»Ÿæ¶æ„å’Œè®¾è®¡å†³ç­–
- [Modern C++ æŒ‡å—](docs/modern_cpp_cuda.md) - ç°ä»£ C++ åœ¨ CUDA ä¸­çš„åº”ç”¨
- [ä¼˜åŒ–æŒ‡å—](docs/optimization_guide.md) - Kernel ä¼˜åŒ–æŠ€æœ¯è¯¦è§£

## ğŸ“„ License

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ Acknowledgments

- NVIDIA CUTLASS - GEMM ä¼˜åŒ–æ¨¡å¼çš„çµæ„Ÿæ¥æº
- FlashAttention - Attention ä¼˜åŒ–æŠ€æœ¯
- PyTorch/TensorFlow - API è®¾è®¡å‚è€ƒ
- CUDA ç¤¾åŒº - æŒç»­çš„å­¦ä¹ èµ„æº

---

<p align="center">
  Made with â¤ï¸ for the HPC and AI community
</p>
